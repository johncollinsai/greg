<!DOCTYPE html>
<html lang="en">
<head>

    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta property="og:image" content="static/images/fine-tuning-llm.png">
    <meta property="og:image:width" content="1200">
    <meta property="og:image:height" content="630">
    <meta name="title" content="johncollins.ai/volllama">
    <meta name="description" content="Fine-tuning LLMs">
    <meta property="og:title" content="johncollins.ai">
    <meta property="og:description" content="Fine-tuning LLMs">
    <meta property="og:url" content="https://johncollins.ai/volllama">
    <meta property="og:type" content="website">
    <meta property="og:site_name" content="johncollins.ai/volllama">
    <meta property="og:image" content="https://johncollins.ai/app/static/images/fine-tuning-llm.png">
    <meta property="og:image:secure_url" content="https://johncollins.ai/app/static/images/fine-tuning-llm.png">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="1200">
    <meta property="og:image:height" content="630">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="johncollins.ai/volllama">
    <meta name="twitter:description" content="Fine-tuning LLMs">
    <meta name="twitter:image" content="https://johncollins.ai/app/static/images/fine-tuning-llm.png">
    <meta name="msapplication-TileColor" content="#ffffff">
    <meta name="msapplication-TileImage" content="https://johncollins.ai/app/static/images/fine-tuning-llm.png">
  
    <title>johncollins.ai</title>
   
    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/5.0.0-alpha1/css/bootstrap.min.css" integrity="sha384-r4NyP46KrjDleawBgD5tp8Y7UzmLA05oM1iAEQ17CSuDqnUK2+k9luXQOfXJCJ4I" crossorigin="anonymous">
    <link rel="stylesheet" href="static/styles.css">
    <link href="https://fonts.googleapis.com/css?family=Quicksand" rel="stylesheet">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.14.0/css/all.css" integrity="sha384-HzLeBuhoNPvSl5KYnjx0BT+WB0QEEqLprO+NBkkk5gbc67FTaL7XIGa2w1L0Xbgc" crossorigin="anonymous">
  
    <!-- JavaScript -->
    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.16.2/dist/umd/popper.min.js" integrity="sha384-ZRQgMf+zD5HtB5x8yhJ4y4Y4YO/7fMeRX/PV/v2t+5Dt7FJ8NpW4LN3ZYpDV7yoA" crossorigin="anonymous"></script>
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/5.0.0-alpha1/css/bootstrap.min.css" integrity="sha384-r4NyP46KrjDleawBgD5tp8Y7UzmLA05oM1iAEQ17CSuDqnUK2+k9luXQOfXJCJ4I" crossorigin="anonymous">
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/5.0.0-alpha1/js/bootstrap.min.js" integrity="sha384-oesi62hOLfzrys4LxRF63OJCXdXDipiYWBnvTl9Y9/TRlw5xlKIEHpNyvvDShgf/" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.js"></script>
  
    <!-- Favicon -->
    <link rel="apple-touch-icon" sizes="180x180" href="static/images/fine-tuning-llm-favicon/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="static/images/fine-tuning-llm-favicon/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="static/images/fine-tuning-llm-favicon/favicon-16x16.png">
    <link rel="manifest" href="static/images/fine-tuning-llm-favicon/site.webmanifest">
    <link rel="shortcut icon" type="image/x-icon" href="static/images/fine-tuning-llm-favicon/favicon.ico">
    <a href="https://johncollins.ai/volllama"><img src="https://johncollins.ai/static/images/fine-tuning-llm-favicon/favicon.ico" alt="Favicon" style="display:none;"></a>
  
    <!-- Include CSS file -->
    <link rel="stylesheet" href="/static/css/style.css">
  
    <!-- Include error handling file -->
    <script src="/static/js/errorHandling.js"></script>
  
    <!-- Include JavaScript file -->
    <script src="/static/js/scripts.js"></script>
    
    <!-- Include Google Fonts -->
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap" rel="stylesheet">

</head>

<body id="myPage" data-spy="scroll" data-target=".navbar" data-offset="60">

<!-- navbar -->
<nav class="navbar navbar-expand-lg fixed-top navbar-custom">
  <div class="container">
    <a class="navbar-brand ml-2 text-light" href="{{ url_for('index') }}">johncollins.ai</a>
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>
    <div class="collapse navbar-collapse" id="navbarNav">
      <ul class="navbar-nav ml-auto">
        <li class="nav-item active">
          <a class="nav-link text-light" href="https://twitter.com/johncollinsai" title="Follow me on Twitter" target="_blank"><i class="fab fa-twitter fa-lg fa-inverse fa-fw"></i></a>
        </li>
        <li class="nav-item active">
          <a class="nav-link text-light" href="https://www.linkedin.com/in/johncollins-ai/" title="Connect with me on LinkedIn" target="_blank"><i class="fab fa-linkedin fa-lg fa-fw"></i></a>
        </li>
        <li class="nav-item ml-2">
          <a href="https://github.com/johncollinsai"><img src="https://github.githubassets.com/favicon.ico" title="Follow me on GitHub" alt="GitHub icon" style="filter: invert(100%);" target="_blank"></a>
        </li>
      </ul>
    </div>
  </div>
</nav>

<!-- Title -->
<div class="jumbotron jumbotron-fluid jumbotron-black-top">
  <div class="container">
    <h1 class="display-4 text-center text-blue">Fine-tuning LLMs</h1>
  </div>
</div>

<!-- TL;DR -->
<div class="container">
  <h2 class="margin-top-custom text-white">TL;DR</h2>
  <p>LLMs increasingly dominate artificial intelligence. As always, I will present a specific example of fine-tuning a LLM to illustrate what I wish to say and provide full access to the source code on <a href="https://github.com/johncollinsai/volllama" target-"_blank">my GitHub</a>.  However, for now, I want to set the scene. In this post I set out various approaches to leveraging pretrained LLMs for new tasks, including in-context learning, finetuning, parameter-efficient finetuning, quantization, and reinforcement learning with human feedback. These approaches offer different ways to optimize model performance while minimizing computational costs.</p>
  <ul>
    <li>Finetuning all layers of a pretrained LLM remains the preferred approach for adapting models to new target tasks, but alternative strategies provide resource-efficient alternatives.</li>
    <li>In-context learning allows for task-specific interactions without direct model access, while indexing enables LLMs to function as information retrieval systems.</li>
    <li>Feature-based approaches offer effective means of adapting LLMs.</li>
    <li>Parameter-efficient finetuning techniques optimize performance while minimizing computational costs, making them especially valuable for larger models.</li>
    <li>Reinforcement learning with human feedback enhances LLM performance by incorporating human preferences through a combination of supervised and reinforcement learning methods.</li>
  </ul>
</div>

<!-- Intro -->
<div class="container">
  <h2 class="margin-top-custom text-white">Introduction</h2>
  <p>The growing significance of large language models (LLMs) in the field of artificial intelligence has led to an increasing need for efficient and effective utilization strategies. Two prominent methods for leveraging pretrained LLMs in new tasks are in-context learning and finetuning. In-context learning enables specific or novel tasks without further training by providing target examples through input prompts. On the other hand, finetuning involves adapting a pretrained LLM to a target task, and this post explores three conventional approaches to achieve this. Additionally, it delves into parameter-efficient finetuning techniques and reinforcement learning with human feedback as alternative strategies for optimizing LLM performance.</p>
</div>

<!-- In-Context Learning and Indexing -->
<div class="container">
  <h2 class="margin-top-custom text-white">In-Context Learning and Indexing</h2>
  <p>In-context learning allows users to leverage pretrained LLMs for specific tasks without requiring direct access to the model. By providing a few task-related examples in the input prompt, the LLM can generate appropriate responses. In situations where direct model access is limited, such as through an API, in-context learning is the obvious approach. An extension of this concept is hard prompt tuning, which involves modifying input words to improve output quality. Alternatively, soft prompt tuning offers a differentiable version of prompt modification.</p>
</div>

<!-- Finetuning Approaches -->
<div class="container">
  <h2 class="margin-top-custom text-white">Finetuning Approaches</h2>
  <p>Finetuning enables adaptation of pretrained LLMs to target tasks by updating model parameters. The feature-based approach involves extracting output embeddings from the pretrained LLM, using them as input features for training a separate classification model. This approach, commonly used for embedding-focused models like BERT, can also be applied to generative models like GPT. A popular variation of the feature-based approach focuses on training only the newly added output layers while keeping the pretrained LLM parameters frozen. An alternative approach entails updating all layers of the pretrained LLM to achieve superior modeling performance. Although it incurs higher computational costs, it generally outperforms. The choice between the different approaches depends on the target task and domain similarity to the pretrained dataset.</p>
</div>

<!-- Parameter-Efficient Finetuning -->
<div class="container">
  <h2 class="margin-top-custom text-white">Parameter-Efficient Finetuning</h2>
  <p>Parameter-efficient finetuning techniques (PEFT) are valuable for optimizing LLM performance while minimizing computational and resource requirements. These techniques introduce a small number of additional parameters for finetuning, reducing the need to train the entire model. Approaches such as prefix tuning, adapters, and low-rank adaptation modify multiple layers, yielding better predictive performance at a low cost. PEFT is particularly relevant when working with larger models that strain GPU memory capacity.</p>
</div>

<!-- Quantization -->
<div class="container">
  <h2 class="margin-top-custom text-white">Quantization</h2>
  <p>A quantized LLM is a compressed version of the original LLM that uses lower-precision data types for weights and activations. There are several approaches to model quantization for LLMs, including post-training quantization, quantization-aware training, and hybrid quantization. Post-training quantization involves training the LLM using floating-point data types and then quantizing the weights and activations to lower-precision data types post-training. Quantization-aware training involves quantizing the LLM during training, allowing the model to adapt to the reduced precision during training. Hybrid quantization combines both post-training quantization and quantization-aware training, allowing the LLM to adapt to lower-precision data types while also achieving good accuracy. Quantization can significantly improve the efficiency of LLM inference, enabling better real-time inference on large-scale datasets.</p>
</div>

<!-- Reinforcement Learning with Human Feedback -->
<div class="container">
  <h2 class="margin-top-custom text-white">Reinforcement Learning with Human Feedback</h2>
  <p>Reinforcement Learning with Human Feedback (RLHF) combines supervised learning and reinforcement learning to finetune pretrained LLMs. Humans provide feedback in the form of ranking or rating different model outputs, which is then used to train a reward model. The reward model guides the adaptation of the pretrained LLM to human preferences using reinforcement learning algorithms such as proximal policy optimization. This approach addresses the challenge of obtaining real-time feedback from humans, enhancing model performance in a more efficient manner.</p>
</div>

<!-- Conclusion -->
<div class="container">
  <h2 class="margin-top-custom text-white">Conclusion</h2>
  <p>LLMs increasingly dominate artificial intelligence, and there is a growing need for efficient and effective utilization strategies. In this post I have explored various approaches to leveraging pretrained LLMs for new tasks, including in-context learning, finetuning, parameter-efficient finetuning, quantization, and reinforcement learning with human feedback. These approaches offer valuable alternatives to finetuning all layers of a pretrained LLM, enabling users to optimize model performance while minimizing computational costs.</p>
</div>

<!-- Next steps -->
<div class="container">
  <h2 class="margin-top-custom text-white">Next steps</h2>
    <p>I will train an open-source LLM, an implementation of LLaMA probably, on trades data and explore various risk management activities. In the set up I envisage, the LLM will replace the typical risk management stack that comprises ETL-database-analytics-visualisation.</p>
</div>

<!-- Source code -->
<div class="container">
  <h2 class="margin-top-custom text-purple">Source code</h4>
  <p>Source code for this post can be found on my <a href="https://github.com/johncollinsai/volllama" target-"_blank">GitHub</a>.</p>
</div>

<!-- References -->
<div class="container">
  <h2 class="margin-top-custom text-white">References</h2>
  <p>Chip Huyen (2023). Building LLM applications for production. <a href="https://huyenchip.com/2023/04/11/llm-engineering.html" target-"_blank">Chip Huyen</a>.</p>
  <p>Hyung Won Chung (2023). <a href="https://docs.google.com/presentation/d/13Tylt2SvKvBL2hgILy5CmBtPDv3rXlVrQp01OzAe5Xo/edit#slide=id.p" target-"_blank">Instruction finetuning and Reinforcement Learning with Human Feedback (RHLF)</a></p>
</div>


<!-- Scripts -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.16.0/umd/popper.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js"></script>

<!-- Footer -->
<div class="container-fluid bg-black py-3">
  <div class="row">
    <div class="col text-center text-white">
      <p>&copy; 2023 johncollins</p>
    </div>
  </div>
</div>
  
</body>
</html>

                           
